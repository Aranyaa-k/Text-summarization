{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0e610c",
   "metadata": {},
   "source": [
    "# Text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e9154",
   "metadata": {},
   "source": [
    "## Text summarization for regular text input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c055ae",
   "metadata": {},
   "source": [
    "## Steps to follow to generate text summary using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1097e",
   "metadata": {},
   "source": [
    "- Step 1: Text cleaning and tokenization - stop word and punctuation removal and tokenization of the text\n",
    "- Step 2: Word frequency table - finding the frequency of each word token\n",
    "- Step 3: Sentence tokenization - breaking the text into distinct sentences \n",
    "- Step 4: Calculating sentence score - generating score for each sentence token\n",
    "- Step 5: Summary generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02961402",
   "metadata": {},
   "source": [
    "### Input text - History of Big Bang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912a151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Maya went to London on 15th July. I am a bag of space ducks. That is the masterpiece that is produced by my brain.\n",
    "Hello world is the first code that is written my any student that start learning coding. I like reading books.\n",
    "I need acting leaves to kill buildings. Folders and chargers dance on the cloud. There is no connection here and this is a test.\n",
    "Baby bottles can open doors. All these sentences are suggested by my Shreya shri ragi. Billboards can be distracting will driving.\n",
    "My back hurts. Don't forget to eat snakes and snort waffles. Dustbins drool on floors. \n",
    "This is a very random paragraph that has no connection between the sentences. Put a cactus up your association. Keyboards are not keys.\n",
    "Bucket handles cannot but put in your nose. Fruit loops for breakfast. Apples are red. Boxes stick to trees. Curtains are white. Veg meals for lunch.\n",
    "I love music and art. Sleeping late is not a goood habit. I have amazing friends. It is good to save electricity. Bangalore is a big city.\n",
    "This is to make it so that there are atleast two chunks in the summary so we can see the output. This is the last review for the internship. \n",
    "The German Johannes Gutenberg introduced printing in Europe. Before understanding how to become an ethical hacker, let us understand more about the role.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57f89c",
   "metadata": {},
   "source": [
    "### Installing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7819a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\anaconda\\lib\\site-packages (3.3.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.4.0-cp39-cp39-win_amd64.whl (11.8 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in d:\\anaconda\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.0-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in d:\\anaconda\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\anaconda\\lib\\site-packages (from spacy) (2.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-md 3.3.0 requires spacy<3.4.0,>=3.3.0.dev0, but you have spacy 3.4.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in d:\\anaconda\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.8-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: blis, thinc, spacy\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 0.7.7\n",
      "    Uninstalling blis-0.7.7:\n",
      "      Successfully uninstalled blis-0.7.7\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.0.17\n",
      "    Uninstalling thinc-8.0.17:\n",
      "      Successfully uninstalled thinc-8.0.17\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.3.1\n",
      "    Uninstalling spacy-3.3.1:\n",
      "      Successfully uninstalled spacy-3.3.1\n",
      "Successfully installed blis-0.7.8 spacy-3.4.0 thinc-8.1.0\n",
      "Collecting en-core-web-md==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.0/en_core_web_md-3.4.0-py3-none-any.whl (42.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in d:\\anaconda\\lib\\site-packages (from en-core-web-md==3.4.0) (3.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.20.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (58.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.26.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (21.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.62.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\anaconda\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "  Attempting uninstall: en-core-web-md\n",
      "    Found existing installation: en-core-web-md 3.3.0\n",
      "    Uninstalling en-core-web-md-3.3.0:\n",
      "      Successfully uninstalled en-core-web-md-3.3.0\n",
      "Successfully installed en-core-web-md-3.4.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 18:02:48.412731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-07-18 18:02:48.412873: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-18 18:03:18.997092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-07-18 18:03:19.014906: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2022-07-18 18:03:19.019050: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2022-07-18 18:03:19.022675: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2022-07-18 18:03:19.026516: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2022-07-18 18:03:19.030827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\n",
      "2022-07-18 18:03:19.034092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2022-07-18 18:03:19.036513: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2022-07-18 18:03:19.036618: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy \n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb1586",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d951e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3978f93",
   "metadata": {},
   "source": [
    "### Step 1 - Text cleaning and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017859db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of stop words of the english language\n",
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f6fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the trained spacy model pipeline \n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a1ee2",
   "metadata": {},
   "source": [
    "The below code cell generates the tokens in the document but it also includes stop words and punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a488c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maya', 'went', 'to', 'London', 'on', '15th', 'July', '.', 'I', 'am', 'a', 'bag', 'of', 'space', 'ducks', '.', 'That', 'is', 'the', 'masterpiece', 'that', 'is', 'produced', 'by', 'my', 'brain', '.', '\\n', 'Hello', 'world', 'is', 'the', 'first', 'code', 'that', 'is', 'written', 'my', 'any', 'student', 'that', 'start', 'learning', 'coding', '.', 'I', 'like', 'reading', 'books', '.', '\\n', 'I', 'need', 'acting', 'leaves', 'to', 'kill', 'buildings', '.', 'Folders', 'and', 'chargers', 'dance', 'on', 'the', 'cloud', '.', 'There', 'is', 'no', 'connection', 'here', 'and', 'this', 'is', 'a', 'test', '.', '\\n', 'Baby', 'bottles', 'can', 'open', 'doors', '.', 'All', 'these', 'sentences', 'are', 'suggested', 'by', 'my', 'Shreya', 'shri', 'ragi', '.', 'Billboards', 'can', 'be', 'distracting', 'will', 'driving', '.', '\\n', 'My', 'back', 'hurts', '.', 'Do', \"n't\", 'forget', 'to', 'eat', 'snakes', 'and', 'snort', 'waffles', '.', 'Dustbins', 'drool', 'on', 'floors', '.', '\\n', 'This', 'is', 'a', 'very', 'random', 'paragraph', 'that', 'has', 'no', 'connection', 'between', 'the', 'sentences', '.', 'Put', 'a', 'cactus', 'up', 'your', 'association', '.', 'Keyboards', 'are', 'not', 'keys', '.', '\\n', 'Bucket', 'handles', 'can', 'not', 'but', 'put', 'in', 'your', 'nose', '.', 'Fruit', 'loops', 'for', 'breakfast', '.', 'Apples', 'are', 'red', '.', 'Boxes', 'stick', 'to', 'trees', '.', 'Curtains', 'are', 'white', '.', 'Veg', 'meals', 'for', 'lunch', '.', '\\n', 'I', 'love', 'music', 'and', 'art', '.', 'Sleeping', 'late', 'is', 'not', 'a', 'goood', 'habit', '.', 'I', 'have', 'amazing', 'friends', '.', 'It', 'is', 'good', 'to', 'save', 'electricity', '.', 'Bangalore', 'is', 'a', 'big', 'city', '.', '\\n', 'This', 'is', 'to', 'make', 'it', 'so', 'that', 'there', 'are', 'atleast', 'two', 'chunks', 'in', 'the', 'summary', 'so', 'we', 'can', 'see', 'the', 'output', '.', 'This', 'is', 'the', 'last', 'review', 'for', 'the', 'internship', '.', '\\n', 'The', 'German', 'Johannes', 'Gutenberg', 'introduced', 'printing', 'in', 'Europe', '.', 'Before', 'understanding', 'how', 'to', 'become', 'an', 'ethical', 'hacker', ',', 'let', 'us', 'understand', 'more', 'about', 'the', 'role', '.']\n"
     ]
    }
   ],
   "source": [
    "# Generation of tokens from text\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bda0b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding next line to punctuation\n",
    "punctuation = punctuation + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c02a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maya', 'went', 'london', '15th', 'july', 'bag', 'space', 'ducks', 'masterpiece', 'produced', 'brain', 'hello', 'world', 'code', 'written', 'student', 'start', 'learning', 'coding', 'like', 'reading', 'books', 'need', 'acting', 'leaves', 'kill', 'buildings', 'folders', 'chargers', 'dance', 'cloud', 'connection', 'test', 'baby', 'bottles', 'open', 'doors', 'sentences', 'suggested', 'shreya', 'shri', 'ragi', 'billboards', 'distracting', 'driving', 'hurts', 'forget', 'eat', 'snakes', 'snort', 'waffles', 'dustbins', 'drool', 'floors', 'random', 'paragraph', 'connection', 'sentences', 'cactus', 'association', 'keyboards', 'keys', 'bucket', 'handles', 'nose', 'fruit', 'loops', 'breakfast', 'apples', 'red', 'boxes', 'stick', 'trees', 'curtains', 'white', 'veg', 'meals', 'lunch', 'love', 'music', 'art', 'sleeping', 'late', 'goood', 'habit', 'amazing', 'friends', 'good', 'save', 'electricity', 'bangalore', 'big', 'city', 'atleast', 'chunks', 'summary', 'output', 'review', 'internship', 'german', 'johannes', 'gutenberg', 'introduced', 'printing', 'europe', 'understanding', 'ethical', 'hacker', 'let', 'understand', 'role']\n"
     ]
    }
   ],
   "source": [
    "# Generating new tokens by Text cleaning\n",
    "new_tokens = []\n",
    "for word in tokens:\n",
    "    if word.lower() not in stopwords and word not in punctuation:\n",
    "        new_tokens.append(word.lower())\n",
    "print(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99667ec8",
   "metadata": {},
   "source": [
    "### Step 2 - Generating the word frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6435d2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'maya': 1, 'went': 1, 'london': 1, '15th': 1, 'july': 1, 'bag': 1, 'space': 1, 'ducks': 1, 'masterpiece': 1, 'produced': 1, 'brain': 1, 'hello': 1, 'world': 1, 'code': 1, 'written': 1, 'student': 1, 'start': 1, 'learning': 1, 'coding': 1, 'like': 1, 'reading': 1, 'books': 1, 'need': 1, 'acting': 1, 'leaves': 1, 'kill': 1, 'buildings': 1, 'folders': 1, 'chargers': 1, 'dance': 1, 'cloud': 1, 'connection': 2, 'test': 1, 'baby': 1, 'bottles': 1, 'open': 1, 'doors': 1, 'sentences': 2, 'suggested': 1, 'shreya': 1, 'shri': 1, 'ragi': 1, 'billboards': 1, 'distracting': 1, 'driving': 1, 'hurts': 1, 'forget': 1, 'eat': 1, 'snakes': 1, 'snort': 1, 'waffles': 1, 'dustbins': 1, 'drool': 1, 'floors': 1, 'random': 1, 'paragraph': 1, 'cactus': 1, 'association': 1, 'keyboards': 1, 'keys': 1, 'bucket': 1, 'handles': 1, 'nose': 1, 'fruit': 1, 'loops': 1, 'breakfast': 1, 'apples': 1, 'red': 1, 'boxes': 1, 'stick': 1, 'trees': 1, 'curtains': 1, 'white': 1, 'veg': 1, 'meals': 1, 'lunch': 1, 'love': 1, 'music': 1, 'art': 1, 'sleeping': 1, 'late': 1, 'goood': 1, 'habit': 1, 'amazing': 1, 'friends': 1, 'good': 1, 'save': 1, 'electricity': 1, 'bangalore': 1, 'big': 1, 'city': 1, 'atleast': 1, 'chunks': 1, 'summary': 1, 'output': 1, 'review': 1, 'internship': 1, 'german': 1, 'johannes': 1, 'gutenberg': 1, 'introduced': 1, 'printing': 1, 'europe': 1, 'understanding': 1, 'ethical': 1, 'hacker': 1, 'let': 1, 'understand': 1, 'role': 1}\n"
     ]
    }
   ],
   "source": [
    "# Creating word frequency table from new_tokens\n",
    "word_frequency = {}\n",
    "for word in new_tokens:\n",
    "    if word not in word_frequency.keys():\n",
    "        word_frequency[word] = 1\n",
    "    else:\n",
    "        word_frequency[word] += 1\n",
    "print(word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b8f3516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'maya': 0.5, 'went': 0.5, 'london': 0.5, '15th': 0.5, 'july': 0.5, 'bag': 0.5, 'space': 0.5, 'ducks': 0.5, 'masterpiece': 0.5, 'produced': 0.5, 'brain': 0.5, 'hello': 0.5, 'world': 0.5, 'code': 0.5, 'written': 0.5, 'student': 0.5, 'start': 0.5, 'learning': 0.5, 'coding': 0.5, 'like': 0.5, 'reading': 0.5, 'books': 0.5, 'need': 0.5, 'acting': 0.5, 'leaves': 0.5, 'kill': 0.5, 'buildings': 0.5, 'folders': 0.5, 'chargers': 0.5, 'dance': 0.5, 'cloud': 0.5, 'connection': 1.0, 'test': 0.5, 'baby': 0.5, 'bottles': 0.5, 'open': 0.5, 'doors': 0.5, 'sentences': 1.0, 'suggested': 0.5, 'shreya': 0.5, 'shri': 0.5, 'ragi': 0.5, 'billboards': 0.5, 'distracting': 0.5, 'driving': 0.5, 'hurts': 0.5, 'forget': 0.5, 'eat': 0.5, 'snakes': 0.5, 'snort': 0.5, 'waffles': 0.5, 'dustbins': 0.5, 'drool': 0.5, 'floors': 0.5, 'random': 0.5, 'paragraph': 0.5, 'cactus': 0.5, 'association': 0.5, 'keyboards': 0.5, 'keys': 0.5, 'bucket': 0.5, 'handles': 0.5, 'nose': 0.5, 'fruit': 0.5, 'loops': 0.5, 'breakfast': 0.5, 'apples': 0.5, 'red': 0.5, 'boxes': 0.5, 'stick': 0.5, 'trees': 0.5, 'curtains': 0.5, 'white': 0.5, 'veg': 0.5, 'meals': 0.5, 'lunch': 0.5, 'love': 0.5, 'music': 0.5, 'art': 0.5, 'sleeping': 0.5, 'late': 0.5, 'goood': 0.5, 'habit': 0.5, 'amazing': 0.5, 'friends': 0.5, 'good': 0.5, 'save': 0.5, 'electricity': 0.5, 'bangalore': 0.5, 'big': 0.5, 'city': 0.5, 'atleast': 0.5, 'chunks': 0.5, 'summary': 0.5, 'output': 0.5, 'review': 0.5, 'internship': 0.5, 'german': 0.5, 'johannes': 0.5, 'gutenberg': 0.5, 'introduced': 0.5, 'printing': 0.5, 'europe': 0.5, 'understanding': 0.5, 'ethical': 0.5, 'hacker': 0.5, 'let': 0.5, 'understand': 0.5, 'role': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the frequency table \n",
    "max_freq = max(word_frequency.values())\n",
    "for word in word_frequency.keys():\n",
    "    word_frequency[word] = word_frequency[word]/max_freq\n",
    "print(word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e38bcfd",
   "metadata": {},
   "source": [
    "### Step 3 - Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45112741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Maya went to London on 15th July., I am a bag of space ducks., That is the masterpiece that is produced by my brain.\n",
      ", Hello world is the first code that is written my any student that start learning coding., I like reading books.\n",
      ", I need acting leaves to kill buildings., Folders and chargers dance on the cloud., There is no connection here and this is a test.\n",
      ", Baby bottles can open doors., All these sentences are suggested by my Shreya shri ragi., Billboards can be distracting will driving.\n",
      ", My back hurts., Don't forget to eat snakes and snort waffles., Dustbins drool on floors. \n",
      ", This is a very random paragraph that has no connection between the sentences., Put a cactus up your association., Keyboards are not keys.\n",
      ", Bucket handles cannot but put in your nose., Fruit loops for breakfast., Apples are red., Boxes stick to trees., Curtains are white., Veg meals for lunch.\n",
      ", I love music and art., Sleeping late is not a goood habit., I have amazing friends., It is good to save electricity., Bangalore is a big city.\n",
      ", This is to make it so that there are atleast two chunks in the summary so we can see the output., This is the last review for the internship. \n",
      ", The German Johannes Gutenberg introduced printing in Europe., Before understanding how to become an ethical hacker, let us understand more about the role.]\n"
     ]
    }
   ],
   "source": [
    "# Splitting text to sentences\n",
    "sentence_tokens = [s for s in doc.sents]\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7136e5",
   "metadata": {},
   "source": [
    "### Step 4 - Calculating the sentence score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8546ac",
   "metadata": {},
   "source": [
    "Each sentence in the document is given a score. The score is calculated be summing the normalized word token frequencies of each word in the given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dde47328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Maya went to London on 15th July.: 2.5, I am a bag of space ducks.: 1.5, That is the masterpiece that is produced by my brain.\n",
      ": 1.5, Hello world is the first code that is written my any student that start learning coding.: 4.0, I like reading books.\n",
      ": 1.5, I need acting leaves to kill buildings.: 2.5, Folders and chargers dance on the cloud.: 2.0, There is no connection here and this is a test.\n",
      ": 1.5, Baby bottles can open doors.: 2.0, All these sentences are suggested by my Shreya shri ragi.: 3.0, Billboards can be distracting will driving.\n",
      ": 1.5, My back hurts.: 0.5, Don't forget to eat snakes and snort waffles.: 2.5, Dustbins drool on floors. \n",
      ": 1.5, This is a very random paragraph that has no connection between the sentences.: 3.0, Put a cactus up your association.: 1.0, Keyboards are not keys.\n",
      ": 1.0, Bucket handles cannot but put in your nose.: 1.5, Fruit loops for breakfast.: 1.5, Apples are red.: 1.0, Boxes stick to trees.: 1.5, Curtains are white.: 1.0, Veg meals for lunch.\n",
      ": 1.5, I love music and art.: 1.5, Sleeping late is not a goood habit.: 2.0, I have amazing friends.: 1.0, It is good to save electricity.: 1.5, Bangalore is a big city.\n",
      ": 1.5, This is to make it so that there are atleast two chunks in the summary so we can see the output.: 2.0, This is the last review for the internship. \n",
      ": 1.0, The German Johannes Gutenberg introduced printing in Europe.: 3.0, Before understanding how to become an ethical hacker, let us understand more about the role.: 3.0}\n"
     ]
    }
   ],
   "source": [
    "sentence_score = {}\n",
    "for sentence in sentence_tokens:\n",
    "    for word in sentence:\n",
    "        if word.text.lower() in word_frequency.keys():\n",
    "            if sentence not in sentence_score.keys():\n",
    "                sentence_score[sentence] = word_frequency[word.text.lower()]\n",
    "            else:\n",
    "                sentence_score[sentence] += word_frequency[word.text.lower()]\n",
    "print(sentence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c5f68",
   "metadata": {},
   "source": [
    "### Step 5 - Obtaining the summary of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fb7f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting of 30% of the total text that will be the summary\n",
    "from heapq import nlargest\n",
    "select_length = int(len(sentence_tokens)*0.3) # 0.3 for 30%. The value can be changed as per requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed8fde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Hello world is the first code that is written my any student that start learning coding.,\n",
       " All these sentences are suggested by my Shreya shri ragi.,\n",
       " This is a very random paragraph that has no connection between the sentences.,\n",
       " The German Johannes Gutenberg introduced printing in Europe.,\n",
       " Before understanding how to become an ethical hacker, let us understand more about the role.,\n",
       " Maya went to London on 15th July.,\n",
       " I need acting leaves to kill buildings.,\n",
       " Don't forget to eat snakes and snort waffles.,\n",
       " Folders and chargers dance on the cloud.]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating the summary\n",
    "summary = nlargest(select_length, sentence_score, key = sentence_score.get)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33d64a",
   "metadata": {},
   "source": [
    "## Text summarization for websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b12796",
   "metadata": {},
   "source": [
    "### Installing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac8c2965",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Using cached sumy-0.10.0-py2.py3-none-any.whl (94 kB)\n",
      "Collecting breadability>=0.1.20\n",
      "  Using cached breadability-0.1.20.tar.gz (32 kB)\n",
      "Collecting pycountry>=18.2.23\n",
      "  Using cached pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: nltk>=3.0.2 in d:\\anaconda\\lib\\site-packages (from sumy) (3.6.5)\n",
      "Requirement already satisfied: requests>=2.7.0 in d:\\anaconda\\lib\\site-packages (from sumy) (2.26.0)\n",
      "Collecting docopt<0.7,>=0.6.1\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: chardet in d:\\anaconda\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in d:\\anaconda\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.6.3)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.0.3)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk>=3.0.2->sumy) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.62.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from pycountry>=18.2.23->sumy) (58.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.7.0->sumy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.7.0->sumy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.7.0->sumy) (1.26.7)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.4)\n",
      "Building wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (setup.py): started\n",
      "  Building wheel for breadability (setup.py): finished with status 'done'\n",
      "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21711 sha256=3760cf6896d55fd8fede54e60549ada363bf60d186fbae18b32a5b47114dd461\n",
      "  Stored in directory: c:\\users\\arany\\appdata\\local\\pip\\cache\\wheels\\ba\\9f\\70\\7795228568b81b57a8932755938da9fb1f291b0576752604aa\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=6d4bfaf833afdc8ae8d7870b6d634a91cded8873ca5ca3ae9482462bd85d73ea\n",
      "  Stored in directory: c:\\users\\arany\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "  Building wheel for pycountry (PEP 517): started\n",
      "  Building wheel for pycountry (PEP 517): finished with status 'done'\n",
      "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=ddc4a2456273b1d33169fdbc4607bfb1e230749dafa0c129cf2b919d2a192aff\n",
      "  Stored in directory: c:\\users\\arany\\appdata\\local\\pip\\cache\\wheels\\47\\15\\92\\e6dc85fcb0686c82e1edbcfdf80cfe4808c058813fed0baa8f\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: docopt, pycountry, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e08f0c",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40c7837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab8567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb96d9",
   "metadata": {},
   "source": [
    "### HTML parsing with inbuilt HTML parser in sumy and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8fb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://phys.org/news/2015-12-big-theory.html#:~:text=In%20short%2C%20the%20Big%20Bang,intense%20heat%20called%20a%20Singularity.\"\n",
    "parser = HtmlParser.from_url(url, Tokenizer(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73c6f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DOM with 49 paragraphs>\n"
     ]
    }
   ],
   "source": [
    "print(parser.document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd5a6c",
   "metadata": {},
   "source": [
    "### Generating summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "641eb2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While this is not the only modern theory of how the Universe came into being – for example, there is the Steady State Theory or the Oscillating Universe Theory – it is the most widely accepted and popular. \n",
      "\n",
      "Not only does the model explain the origin of all known matter, the laws of physics, and the large scale structure of the Universe, it also accounts for the expansion of the Universe and a broad range of other phenomena. \n",
      "\n",
      "Through the testing of theoretical principles, experiments involving particle accelerators and high-energy states, and astronomical studies that have observed the deep universe, scientists have constructed a timeline of events that began with the Big Bang and has led to the current state of cosmic evolution. \n",
      "\n",
      "Over the course of the several billion years that followed, the slightly denser regions of the almost uniformly distributed matter of the Universe began to become gravitationally attracted to each other. \n",
      "\n",
      "However, the Lambda-Cold Dark Matter model (Lambda-CDM), in which the dark matter particles moved slowly compared to the speed of light, is the considered to be the standard model of Big Bang cosmology, as it best fits the available data. \n",
      "\n",
      "With the acceptance of the Big Bang Theory, but prior to the observation of Dark Energy in the 1990s, cosmologists had come to agree on two scenarios as being the most likely outcomes for our Universe. \n",
      "\n",
      "Modern observations, which include the existence of Dark Energy and its influence on cosmic expansion, have led to the conclusion that more and more of the currently visible universe will pass beyond our event horizon (i.e. the CMB, the edge of what we can see) and become invisible to us. \n",
      "\n",
      "And then in 1927, Georges Lemaitre, a Belgian physicist and Roman Catholic priest, independently derived the same results as Friedmann's equations and proposed that the inferred recession of the galaxies was due to the expansion of the universe. \n",
      "\n",
      "The discovery and confirmation of the cosmic microwave background radiation in 1965 secured the Big Bang as the best theory of the origin and evolution of the universe. \n",
      "\n",
      "Today, cosmologists have fairly precise and accurate measurements of many of the parameters of the Big Bang model, not to mention the age of the Universe itself. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarizer = TextRankSummarizer()\n",
    "for sentence in summarizer(parser.document, 10): # Summary will containg 10 sentences \n",
    "    print(sentence, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
